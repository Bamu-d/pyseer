#!/usr/bin/env python
'''Python reimplementation of SEER for bacterial GWAS

Copyright 2017 Marco Galardini'''

def get_options():
    import argparse

    description = 'SEER (doi: 10.1038/ncomms12797), reimplemented in python'
    parser = argparse.ArgumentParser(description=description)

    parser.add_argument('kmers',
                        help='Kmers file')
    parser.add_argument('phenotypes',
                        help='Phenotypes file')
    parser.add_argument('distances',
                        help='Strains distance square matrix')

    parser.add_argument('--continuous',
                        action='store_true',
                        default=False,
                        help='Continuous phenotype [Default: binary]')
    parser.add_argument('--min-af',
                        type=float,
                        default=0.01,
                        help='Minimum AF [Default: 0.01]')
    parser.add_argument('--max-af',
                        type=float,
                        default=0.99,
                        help='Maximum AF [Default: 0.99]')
    parser.add_argument('--filter-pvalue',
                        type=float,
                        default=1,
                        help='Prefiltering t-test pvalue threshold [Default: 1]')
    parser.add_argument('--lrt-pvalue',
                        type=float,
                        default=1,
                        help='Likelihood ratio test pvalue threshold [Default: 1]')
    parser.add_argument('--max-dimensions',
                        type=int,
                        default=10,
                        help='Maximum number of dimensions to consider after MDS [Default: 10]')
    parser.add_argument('--uncompressed',
                        action='store_true',
                        default=False,
                        help='Uncompressed kmers file [Default: gzipped]')
    parser.add_argument('--cpu',
                        type=int,
                        default=1,
                        help='Processes [Default: 1]')

    return parser.parse_args()


# thanks to Francis Song for this function
# source: http://www.nervouscomputer.com/hfs/cmdscale-in-python/
def cmdscale(D):
    """
    Classical multidimensional scaling (MDS)

    Parameters
    ----------
    D : (n, n) array
        Symmetric distance matrix.

    Returns
    -------
    Y : (n, p) array
        Configuration matrix. Each column represents a dimension. Only the
        p dimensions corresponding to positive eigenvalues of B are returned.
        Note that each dimension is only determined up to an overall sign,
        corresponding to a reflection.

    e : (n,) array
        Eigenvalues of B.
    """
    # Number of points
    n = len(D)

    # Centering matrix
    H = np.eye(n) - np.ones((n, n))/n

    # YY^T
    B = -H.dot(D**2).dot(H)/2

    # Diagonalize
    evals, evecs = np.linalg.eigh(B)

    # Sort by eigenvalue in descending order
    idx   = np.argsort(evals)[::-1]
    evals = evals[idx]
    evecs = evecs[:,idx]

    # Compute the coordinates using positive-eigenvalued components only
    w, = np.where(evals > 0)
    L  = np.diag(np.sqrt(evals[w]))
    V  = evecs[:,w]
    Y  = V.dot(L)

    return Y, evals


def iter_kmers(p, m, infile, all_strains,
               min_af, max_af,
               filter_pvalue, lrt_pvalue, null_fit):
    for l in infile:
        if not options.uncompressed:
            l = l.decode()
        kmer, strains = l.split()[0], l.rstrip().split()[2:]
        af = len(strains) / len(all_strains)
        # filter by AF
        if af < min_af or af > max_af:
            continue

        k = {x.split(':')[0]: int(x.split(':')[1])
             for x in strains}
        for x in all_strains.difference({x.split(':')[0]
                                         for x in strains}):
            k[x] = 0

        k = pd.Series(k, name=kmer)
        k[np.isnan(k)] = 0.0
        k[k >= 1] = 1.0
        yield p, k, m, af, filter_pvalue, lrt_pvalue, null_fit


def binary(p, k, m, af, pret, lrtt, null_res):
    # pre-filtering
    t = p.to_frame().join(k.to_frame(), how='inner')
    table = [[t[(t[0] == 1) & (t[k.name] == 1)].shape[0],
              t[(t[0] == 1) & (t[k.name] == 0)].shape[0]],
             [t[(t[0] == 0) & (t[k.name] == 1)].shape[0],
              t[(t[0] == 0) & (t[k.name] == 0)].shape[0]]]

    # check for small values
    bad_chisq = 1
    bad_entries =0
    for row in table:
        for entry in row:
            if entry <= 1:
                bad_chisq = 1
            else if entry <= 5:
                bad_entries += 1
    if bad_entries > 1:
        bad_chisq = 1

    prep = stats.chi2_contingency(table, correction=False)[1]
    if prep > pret:
        return None

    # actual logistic regression
    v = np.concatenate((p.values.reshape(-1, 1),
                        k.values.reshape(-1, 1),
                        m.values),
                       axis=1)
    df = pd.DataFrame(v,
                      columns=['phenotype', 'kmer'] +
                              ['PC%d'%x for x in range(1, m.shape[1]+1)])
    mod1 = smf.logit(formula='phenotype ~ kmer + ' +
                             ' + '.join(['PC%d'%x for x in range(1, m.shape[1]+1)]),
                     data=df)

    # suppress annoying stdout messages
    old_stdout = sys.stdout
    sys.stdout = open(os.devnull, "w")
    try:
        if not bad_chisq:
            # try first bfgs optimization
            res1 = mod1.fit(method='bfgs')
            if not res1.mle_retvals['converged']:
                # fallback to Newton-Raphson
                res1 = mod1.fit(method='newton')
            fitll = res1.llf

        # Fit Firth regression with large SE, or nearly separable values
        if res1.bse.kmer > 3 or bad_chisq:
            fitll, waldp = firth(df)
        else:
            waldp = float(res1.wald_test('kmer = 0').pvalue)
    except np.linalg.linalg.LinAlgError:
        # singular matrix error
        os.stderr.write("Matrix inversion error with kmer " + str(k))
        return None
    finally:
        sys.stdout.close()
        sys.stdout = old_stdout

    # lrt test for logit
    df_full = res1.df_resid
    df_null = null_res.df_resid
    lrdf = (df_null - df_full)
    lrstat = -2*(null_res.llf - fitll)
    lrt_pvalue = stats.chi2.sf(lrstat, lrdf)

    if lrt_pvalue > lrtt:
        return None

    x = '\t'.join([str(x)
                   for x in [k.name,
                             af,
                             prep,
                             waldp,
                             lrt_pvalue,
                             res1.params.kmer,
                             res1.bse.kmer,
                             res1.params.Intercept] +
                             [getattr(res1.params, 'PC%d'%x) for x in range(1, m.shape[1]+1)] +
                             [','.join(sorted(k[k == 1].index)),
                              ','.join(sorted(k[k == 0].index))]])
    return x

def firth(df):

    # Do firth regression
    logit_model = smf.logit(formula='phenotype ~ kmer + ' +
                             ' + '.join(['PC%d'%x for x in range(1, m.shape[1]+1)]),
                     data=df)
    #TODO
    beta = optimise(firth_likelihood, logit_model)

    fitll = firth_likelihood(beta, logit_model)

    # Calculate Wald p-value
    se = sqrt(logit.information(beta)[1])
    wald = abs(beta[1]/se)
    waldp = 2* (1-stats.norm.cdf(wald))

    return fitll, waldp

# This can be a lambda
def firth_likelihood(beta, logit):
    return logit.loglike(beta) + 0.5*np.log(np.trace(logit.information(beta)))


def continuous(p, k, m, af, pret, lrtt, null_res):
    # pre-filtering
    prep = stats.ttest_ind(p.loc[p.index[k == 1]],
                           p.loc[p.index[k == 0]],
                           equal_var=False)[1]
    if prep > pret:
        return None

    # actual linear regression
    v = np.concatenate((p.values.reshape(-1, 1),
                        k.values.reshape(-1, 1),
                        m.values),
                       axis=1)
    df = pd.DataFrame(v,
                      columns=['phenotype', 'kmer'] +
                      ['PC%d'%x for x in range(1, m.shape[1]+1)])
    mod1 = smf.ols(formula='phenotype ~ kmer + ' +
                           ' + '.join(['PC%d'%x for x in range(1, m.shape[1]+1)]),
                   data=df)

    # suppress annoying stdout messages
    old_stdout = sys.stdout
    sys.stdout = open(os.devnull, "w")
    try:
        res1 = mod1.fit()
    except np.linalg.linalg.LinAlgError:
        # singular matrix error
        os.stderr.write("Matrix inversion error with kmer " + str(k))
        return None
    finally:
        sys.stdout.close()
        sys.stdout = old_stdout

    lrt_pvalue = res1.compare_lr_test(null_res)[1]
    if lrt_pvalue > lrtt:
        return None

    x = '\t'.join([str(x)
                   for x in [k.name,
                             af,
                             prep,
                             float(res1.wald_test('kmer = 0').pvalue),
                             lrt_pvalue,
                             res1.params.kmer,
                             res1.bse.kmer,
                             res1.params.Intercept] +
                             [getattr(res1.params, 'PC%d'%x) for x in range(1, m.shape[1]+1)] +
                             [','.join(sorted(k[k == 1].index)),
                             ','.join(sorted(k[k == 0].index))]])
    return x

# Fit the null model, regression without k-mer
def fit_null(p, m, continuous):
    v = np.concatenate((p.values.reshape(-1, 1),
                        m.values),
                       axis=1)
    df = pd.DataFrame(v,
                      columns=['phenotype'] +
                      ['PC%d'%x for x in range(1, m.shape[1]+1)])
    if continuous:
        null_mod = smf.ols(formula='phenotype ~ ' +
                           ' + '.join(['PC%d'%x for x in range(1, m.shape[1]+1)]),
                   data=df)
    else:
        null_mod = smf.logit(formula='phenotype ~ ' +
                             ' + '.join(['PC%d'%x for x in range(1, m.shape[1]+1)]),
                     data=df)

    # suppress annoying stdout messages
    old_stdout = sys.stdout
    sys.stdout = open(os.devnull, "w")
    try:
        if continuous:
            null_res = null_mod.fit()
        else:
            null_res = null_mod.fit(method='newton')

    except np.linalg.linalg.LinAlgError:
        # singular matrix error
        os.stderr.write("Matrix inversion error with kmer " + str(k))
        return None
    finally:
        sys.stdout.close()
        sys.stdout = old_stdout

    return null_res


if __name__ == "__main__":
    import sys
    if sys.version_info[0] < 3:
        sys.stderr.write('pyseer requires python version 3 or above\n')
        sys.exit(1)

    options = get_options()

    import os
    import gzip
    import warnings
    import itertools
    import numpy as np
    import pandas as pd
    from scipy import stats
    from multiprocessing import Pool
    import statsmodels.formula.api as smf

    # silence warnings
    warnings.filterwarnings('ignore')
    #

    # reading phenotypes
    p = pd.Series([float(x.rstrip().split()[-1])
                   for x in open(options.phenotypes)],
                  index=[x.split()[0]
                         for x in open(options.phenotypes)])

    # reading genome distances
    m = pd.read_table(options.distances,
                      index_col=0)
    m = m.loc[p.index, p.index]
    # metric MDS scaling
    m = pd.DataFrame(cmdscale(m)[0][:, :options.max_dimensions],
                     index=m.index)
    for i in range(m.shape[1]):
        m[i] = m[i] / max(abs(m[i]))

    all_strains = set(p.index)

    print('\t'.join(['kmer', 'af', 'filter-pvalue',
                     'wald-pvalue', 'lrt-pvalue', 'beta', 'beta-std-err',
                     'intercept'] + ['covar_%d' % i for i in range(1, options.max_dimensions+1)] +
                     ['k-samples', 'nk-samples']))
    if options.uncompressed:
        infile = open(options.kmers)
    else:
        infile = gzip.open(options.kmers, 'r')

    # multiprocessing setup
    pool = Pool(options.cpu)

    # calculate null regressions once
    null_fit = fit_null(p, m, options.continuous)

    # iterator over each kmer
    # implements maf filtering
    k_iter = iter_kmers(p, m, infile, all_strains,
                        options.min_af, options.max_af,
                        options.filter_pvalue,
                        options.lrt_pvalue, null_fit)

    # actual association test
    # multiprocessing proceeds 1000 MAF-passing kmers per core at a time
    if options.continuous:
        while True:
            ret = pool.starmap(continuous, itertools.islice(k_iter, options.cpu*1000))
            if not ret:
                break
            for x in ret:
                if x is None:
                    continue
                print(x)
    else:
        while True:
            ret = pool.starmap(binary, itertools.islice(k_iter, options.cpu*1000))
            if not ret:
                break
            for x in ret:
                if x is None:
                    continue
                print(x)
