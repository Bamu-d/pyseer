#!/usr/bin/env python
'''Python reimplementation of SEER for bacterial GWAS

Copyright 2017 Marco Galardini'''

def get_options():
    import argparse

    description = 'SEER (doi: 10.1038/ncomms12797), reimplemented in python'
    parser = argparse.ArgumentParser(description=description)

    parser.add_argument('kmers',
                        help='Kmers file')
    parser.add_argument('phenotypes',
                        help='Phenotypes file')
    parser.add_argument('distances',
                        help='Strains distance square matrix')

    parser.add_argument('--continuous',
                        action='store_true',
                        default=False,
                        help='Continuous phenotype [Default: binary]')
    parser.add_argument('--print-samples',
                        action='store_true',
                        default=False,
                        help='Print sample lists [Default: hide samples]')
    parser.add_argument('--min-af',
                        type=float,
                        default=0.01,
                        help='Minimum AF [Default: 0.01]')
    parser.add_argument('--max-af',
                        type=float,
                        default=0.99,
                        help='Maximum AF [Default: 0.99]')
    parser.add_argument('--filter-pvalue',
                        type=float,
                        default=1,
                        help='Prefiltering t-test pvalue threshold [Default: 1]')
    parser.add_argument('--lrt-pvalue',
                        type=float,
                        default=1,
                        help='Likelihood ratio test pvalue threshold [Default: 1]')
    parser.add_argument('--max-dimensions',
                        type=int,
                        default=10,
                        help='Maximum number of dimensions to consider after MDS [Default: 10]')
    parser.add_argument('--uncompressed',
                        action='store_true',
                        default=False,
                        help='Uncompressed kmers file [Default: gzipped]')
    parser.add_argument('--cpu',
                        type=int,
                        default=1,
                        help='Processes [Default: 1]')
    parser.add_argument('--save-m',
                        help='Prefix for saving matrix decomposition')
    parser.add_argument('--load-m',
                        help='Load an existing matrix decomposition')

    return parser.parse_args()


# thanks to Francis Song for this function
# source: http://www.nervouscomputer.com/hfs/cmdscale-in-python/
def cmdscale(D):
    """
    Classical multidimensional scaling (MDS)

    Parameters
    ----------
    D : (n, n) array
        Symmetric distance matrix.

    Returns
    -------
    Y : (n, p) array
        Configuration matrix. Each column represents a dimension. Only the
        p dimensions corresponding to positive eigenvalues of B are returned.
        Note that each dimension is only determined up to an overall sign,
        corresponding to a reflection.

    e : (n,) array
        Eigenvalues of B.
    """
    # Number of points
    n = len(D)

    # Centering matrix
    H = np.eye(n) - np.ones((n, n))/n

    # YY^T
    B = -H.dot(D**2).dot(H)/2

    # Diagonalize
    evals, evecs = np.linalg.eigh(B)

    # Sort by eigenvalue in descending order
    idx   = np.argsort(evals)[::-1]
    evals = evals[idx]
    evecs = evecs[:,idx]

    # Compute the coordinates using positive-eigenvalued components only
    w, = np.where(evals > 0)
    L  = np.diag(np.sqrt(evals[w]))
    V  = evecs[:,w]
    Y  = V.dot(L)

    return Y, evals


def iter_kmers(p, m, infile, all_strains,
               min_af, max_af, print_samples,
               filter_pvalue, lrt_pvalue, null_fit, firth_null):
    for l in infile:
        if not options.uncompressed:
            l = l.decode()
        kmer, strains = l.split()[0], l.rstrip().split()[2:]
        af = len(strains) / len(all_strains)
        # filter by AF
        if af < min_af or af > max_af:
            continue

        k = {x.split(':')[0]: int(x.split(':')[1])
             for x in strains}
        for x in all_strains.difference({x.split(':')[0]
                                         for x in strains}):
            k[x] = 0

        k = pd.Series(k, name=kmer)
        k[np.isnan(k)] = 0.0
        k[k >= 1] = 1.0
        yield p, k, m, af, print_samples, filter_pvalue, lrt_pvalue, null_fit, firth_null


def binary(p, k, m, af, print_samples, pret, lrtt, null_res, null_firth):
    # pre-filtering
    t = p.to_frame().join(k.to_frame(), how='inner')
    table = [[t[(t[0] == 1) & (t[k.name] == 1)].shape[0],
              t[(t[0] == 1) & (t[k.name] == 0)].shape[0]],
             [t[(t[0] == 0) & (t[k.name] == 1)].shape[0],
              t[(t[0] == 0) & (t[k.name] == 0)].shape[0]]]

    # check for small values
    bad_chisq = 0
    bad_entries = 0
    for row in table:
        for entry in row:
            if entry <= 1:
                bad_chisq = 1
            elif entry <= 5:
                bad_entries += 1
    if bad_entries > 1:
        bad_chisq = 1

    prep = stats.chi2_contingency(table, correction=False)[1]
    if prep > pret:
        return None

    # actual logistic regression
    v = np.concatenate((p.values.reshape(-1, 1),
                        k.values.reshape(-1, 1),
                        m),
                       axis=1)
    df = pd.DataFrame(v,
                      columns=['phenotype', 'kmer'] +
                              ['PC%d'%x for x in range(1, m.shape[1]+1)])
    mod1 = smf.logit(formula='phenotype ~ kmer + ' +
                             ' + '.join(['PC%d'%x for x in range(1, m.shape[1]+1)]),
                     data=df)

    start_vec = np.ones(df.shape[1])
    start_vec[0] = np.log(np.mean(p.values)/(1-np.mean(p.values)))

    # suppress annoying stdout messages
    old_stdout = sys.stdout
    sys.stdout = open(os.devnull, "w")
    try:
        if not bad_chisq:
            # try first bfgs optimization
            res1 = mod1.fit(start_params=start_vec, method='bfgs', maxiter=20)
            if not res1.mle_retvals['converged']:
                # fallback to Newton-Raphson
                res1 = mod1.fit(method='newton')

            if res1.bse.kmer > 3:
                bad_chisq = 1
            else:
                lrstat = -2*(null_res - res1.llf)
                lrt_pvalue = stats.chi2.sf(lrstat, 1)

                intercept = res1.params.Intercept
                kbeta = res1.params.kmer
                beta = [getattr(res1.params, 'PC%d'%x) for x in range(1, m.shape[1]+1)]
                bse = res1.bse.kmer
                waldp = float(res1.wald_test('kmer = 0').pvalue)

        # Fit Firth regression with large SE, or nearly separable values
        if bad_chisq:
            #sys.stderr.write("Firth on " + str(k.name) + "\n")
            X = np.concatenate((np.ones((m.shape[0], 1)),
                       k.values.reshape(-1, 1),
                       m),
                       axis=1)
            firth_fit = fit_firth(mod1, start_vec, k.name, X, p.values)
            if firth_fit is None: # Firth failure
                return None
            else:
                intercept, kbeta, beta, bse, waldp, fitll = firth_fit
                lrstat = -2*(null_firth - fitll)
                lrt_pvalue = stats.chi2.sf(lrstat, 1)

    except np.linalg.linalg.LinAlgError:
        # singular matrix error
        sys.stderr.write("Matrix inversion error with kmer " + str(k.name))
        return None
    finally:
        sys.stdout.close()
        sys.stdout = old_stdout

    if lrt_pvalue > lrtt:
        return None

    output = [k.name, af, prep, waldp, lrt_pvalue, kbeta, bse, intercept] + beta
    if print_samples:
        output = output + [','.join(sorted(k[k == 1].index)),','.join(sorted(k[k == 0].index))]
    x = '\t'.join(format_output(x) for x in output)

    return x

def firth_likelihood(beta, logit):
    return -(logit.loglike(beta) + 0.5*np.log(np.linalg.det(-logit.hessian(beta))))

# Do firth regression
# Note information = -hessian, for some reason available but not implemented in statsmodels
def fit_firth(logit_model, start_vec, kmer_name, X, y, step_limit = 1000, convergence_limit = 0.0001):

    beta_iterations = []
    beta_iterations.append(start_vec)
    for i in range(0, step_limit):
        pi = logit_model.predict(beta_iterations[i])
        W = np.diagflat(np.multiply(pi, 1-pi))
        var_covar_mat = np.linalg.pinv(-logit_model.hessian(beta_iterations[i]))

        # build hat matrix
        rootW = np.sqrt(W)
        H = np.dot(np.transpose(X), np.transpose(rootW))
        H = np.matmul(var_covar_mat, H)
        H = np.matmul(np.dot(rootW, X), H)

        # penalised score
        U = np.matmul(np.transpose(X), p.values - pi + np.multiply(np.diagonal(H), 0.5 - pi))
        new_beta = beta_iterations[i] + np.matmul(var_covar_mat, U)

        # step halving
        j = 0
        while firth_likelihood(new_beta, logit_model) > firth_likelihood(beta_iterations[i], logit_model):
            new_beta = beta_iterations[i] + 0.5*(new_beta - beta_iterations[i])
            j = j + 1
            if (j > step_limit):
                sys.stderr.write("Firth regression failed on kmer " + str(kmer_name) + "\n")
                return None

        beta_iterations.append(new_beta)
        if i > 0 and (np.linalg.norm(beta_iterations[i] - beta_iterations[i-1]) < convergence_limit):
            break

    return_fit = None
    if np.linalg.norm(beta_iterations[i] - beta_iterations[i-1]) >= convergence_limit:
        sys.stderr.write("Firth regression failed on kmer " + str(kmer_name) + "\n")
    else:
        # Calculate stats
        fitll = -firth_likelihood(beta_iterations[-1], logit_model)
        intercept = beta_iterations[-1][0]
        kbeta = beta_iterations[-1][1]
        beta = beta_iterations[-1][2:].tolist()
        bse = math.sqrt(-logit_model.hessian(beta_iterations[-1])[1,1])

        # Calculate p-values
        wald = abs(kbeta/bse)
        waldp = 2 * (1-stats.norm.cdf(wald))

        return_fit = intercept, kbeta, beta, bse, waldp, fitll

    return return_fit


def continuous(p, k, m, af, print_samples, pret, lrtt, null_res):
    # pre-filtering
    prep = stats.ttest_ind(p.loc[p.index[k == 1]],
                           p.loc[p.index[k == 0]],
                           equal_var=False)[1]
    if prep > pret:
        return None

    # actual linear regression
    v = np.concatenate((p.values.reshape(-1, 1),
                        k.values.reshape(-1, 1),
                        m),
                       axis=1)
    df = pd.DataFrame(v,
                      columns=['phenotype', 'kmer'] +
                      ['PC%d'%x for x in range(1, m.shape[1]+1)])
    mod1 = smf.ols(formula='phenotype ~ kmer + ' +
                           ' + '.join(['PC%d'%x for x in range(1, m.shape[1]+1)]),
                   data=df)

    # suppress annoying stdout messages
    old_stdout = sys.stdout
    sys.stdout = open(os.devnull, "w")
    try:
        res1 = mod1.fit()
    except np.linalg.linalg.LinAlgError:
        # singular matrix error
        sys.stderr.write("Matrix inversion error with kmer " + str(k.name))
        return None
    finally:
        sys.stdout.close()
        sys.stdout = old_stdout

    lrt_pvalue = res1.compare_lr_test(null_res)[1]
    if lrt_pvalue > lrtt:
        return None

    output = [k.name, af, prep, float(res1.wald_test('kmer = 0').pvalue), lrt_pvalue, res1.params.kmer, res1.bse.kmer, res1.params.Intercept]
    if print_samples:
        output = output + [getattr(res1.params, 'PC%d'%x) for x in range(1, m.shape[1]+1)]
    x = '\t'.join(format_output(x) for x in output)

    return x

# Fit the null model, regression without k-mer
def fit_null(p, m, continuous, firth = 0):
    v = np.concatenate((p.values.reshape(-1, 1),
                        m),
                       axis=1)
    df = pd.DataFrame(v,
                      columns=['phenotype'] +
                      ['PC%d'%x for x in range(1, m.shape[1]+1)])
    if continuous:
        null_mod = smf.ols(formula='phenotype ~ ' +
                           ' + '.join(['PC%d'%x for x in range(1, m.shape[1]+1)]),
                   data=df)
    else:
        null_mod = smf.logit(formula='phenotype ~ ' +
                             ' + '.join(['PC%d'%x for x in range(1, m.shape[1]+1)]),
                     data=df)

    # suppress annoying stdout messages
    old_stdout = sys.stdout
    sys.stdout = open(os.devnull, "w")
    try:
        if continuous:
            null_res = null_mod.fit()
        else:
            null_res = null_mod.fit(method='newton')
            if firth:
                null_res = -firth_likelihood(null_res.params, null_mod)
            else:
                null_res = null_res.llf

    except np.linalg.linalg.LinAlgError:
        # singular matrix error
        sys.stderr.write("Matrix inversion error with kmer " + str(k))
        return None
    finally:
        sys.stdout.close()
        sys.stdout = old_stdout

    return null_res

def format_output (item):
    formatted = str(item)
    if type(item) is float or type(item) is np.float64:
        formatted = '%.2E' % Decimal(item)

    return formatted

if __name__ == "__main__":
    import sys
    if sys.version_info[0] < 3:
        sys.stderr.write('pyseer requires python version 3 or above\n')
        sys.exit(1)

    options = get_options()

    import os
    import gzip
    import warnings
    import itertools
    import math
    import numpy as np
    import pandas as pd
    from scipy import stats
    from scipy import optimize
    from decimal import Decimal
    from multiprocessing import Pool
    import statsmodels.formula.api as smf

    # silence warnings
    warnings.filterwarnings('ignore')
    #

    # reading phenotypes
    p = pd.Series([float(x.rstrip().split()[-1])
                   for x in open(options.phenotypes)],
                  index=[x.split()[0]
                         for x in open(options.phenotypes)])

    # reading genome distances
    if options.load_m and os.path.isfile(options.load_m):
        m = pd.read_pickle(options.load_m)
        m = m.loc[p.index]
    else:
        m = pd.read_table(options.distances,
                          index_col=0)
        m = m.loc[p.index, p.index]
        # metric MDS scaling
        m = pd.DataFrame(cmdscale(m)[0],
                         index=m.index)
        for i in range(m.shape[1]):
            m[i] = m[i] / max(abs(m[i]))

        if options.save_m:
            m.to_pickle(options.save_m + ".pkl")
    m = m.values[:, :options.max_dimensions]


    all_strains = set(p.index)

    print('\t'.join(['kmer', 'af', 'filter-pvalue',
                     'wald-pvalue', 'lrt-pvalue', 'beta', 'beta-std-err',
                     'intercept'] + ['covar_%d' % i for i in range(1, options.max_dimensions+1)] +
                     ['k-samples', 'nk-samples']))
    if options.uncompressed:
        infile = open(options.kmers)
    else:
        infile = gzip.open(options.kmers, 'r')

    # multiprocessing setup
    pool = Pool(options.cpu)

    # calculate null regressions once
    null_fit = fit_null(p, m, options.continuous)
    if not options.continuous:
        firth_null = fit_null(p, m, options.continuous, 1)

    # iterator over each kmer
    # implements maf filtering
    k_iter = iter_kmers(p, m, infile, all_strains,
                        options.min_af, options.max_af,
                        options.print_samples, options.filter_pvalue,
                        options.lrt_pvalue, null_fit, firth_null)

    # actual association test
    # multiprocessing proceeds 1000 MAF-passing kmers per core at a time
    if options.continuous:
        while True:
            ret = pool.starmap(continuous, itertools.islice(k_iter, options.cpu*1000))
            if not ret:
                break
            for x in ret:
                if x is None:
                    continue
                print(x)
    else:
        while True:
            ret = itertools.starmap(binary, itertools.islice(k_iter, options.cpu*1000))
            if not ret:
                break
            for x in ret:
                if x is None:
                    continue
                print(x)
